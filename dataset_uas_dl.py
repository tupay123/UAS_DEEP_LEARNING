# -*- coding: utf-8 -*-
"""Dataset_UAS-DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lJCtlUiGXYjp4qeQpD0gWYeupOgXYVTc
"""

!pip install --upgrade google-api-python-client pandas

from googleapiclient.discovery import build
import pandas as pd
from google.colab import files

api_key = 'AIzaSyDnXQmFxT3vWcIqCDaK8xckzqalFdG2KBY'
youtube = build('youtube', 'v3', developerKey=api_key)

def get_detailed_comments(video_id, target_count=3500):
    comments_data = []
    nextPageToken = None

    try:
        while len(comments_data) < target_count:
            request = youtube.commentThreads().list(
                part='snippet',
                videoId=video_id,
                maxResults=100,
                pageToken=nextPageToken,
                textFormat='plainText'
            )
            response = request.execute()

            for item in response['items']:
                snippet = item['snippet']['topLevelComment']['snippet']
                comments_data.append({
                    'author': snippet['authorDisplayName'],
                    'ulasan': snippet['textDisplay'],
                    'like': snippet['likeCount'],
                    'publis': snippet['publishedAt']
                })

            nextPageToken = response.get('nextPageToken')
            if not nextPageToken:
                break
        return comments_data
    except Exception as e:
        print(f"Terjadi kesalahan: {e}")
        return comments_data


video_id = 'lFR4utbwliw'
hasil_scraping = get_detailed_comments(video_id)


if len(hasil_scraping) >= 3000:
    df = pd.DataFrame(hasil_scraping)
    df.to_csv('data_komentar_lengkap.csv', index=False)
    print(f"Selesai! Berhasil mengambil {len(df)} data dengan kolom lengkap.")
    print(df.head())
    files.download('data_komentar_lengkap.csv')
else:
    print(f"Data baru terkumpul {len(hasil_scraping)}, minimal harus 3.000 sampel.")

len(df)